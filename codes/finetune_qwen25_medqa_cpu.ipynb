{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5ce88c-018a-4353-822a-c9734fc948c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Ignoring unknown argv passed by environment: ['-f', 'C:\\\\Users\\\\Jaber\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-5fa8135d-4c2d-4c81-b489-a61ed1f53f92.json']\n",
      "[INFO] torch: 2.10.0+cpu\n",
      "[INFO] transformers: 5.1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4398927852649b8bbbaebeba5d9d39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] LoRA target_modules detected: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\n",
      "trainable params: 9,232,384 || all params: 1,552,946,688 || trainable%: 0.5945\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c77296618504fdaa2cfc66d1bd9eead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15586 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4895ea1cda448eab759bd55145d6f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='975' max='975' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [975/975 78:52:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.089835</td>\n",
       "      <td>1.093011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =                1.0\n",
      "  total_flos               =         34264056GF\n",
      "  train_loss               =             1.0867\n",
      "  train_runtime            = 3 days, 6:56:47.67\n",
      "  train_samples_per_second =              0.055\n",
      "  train_steps_per_second   =              0.003\n",
      "[INFO] Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='821' max='821' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [821/821 59:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_loss               =      1.093\n",
      "  eval_runtime            = 0:59:45.62\n",
      "  eval_samples_per_second =      0.229\n",
      "  eval_steps_per_second   =      0.229\n",
      "  perplexity              =     2.9832\n",
      "[INFO] Saving LoRA adapter to: ./qwen25_1p5b_medqa_lora_cpu\\adapter\n",
      "[DONE] Training complete.\n",
      "[DONE] Adapter saved at: ./qwen25_1p5b_medqa_lora_cpu\\adapter\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CPU-only LoRA fine-tuning for:\n",
    "  Base model: Qwen/Qwen2.5-1.5B-Instruct\n",
    "  Dataset:    Medical_QA_Dataset.csv with columns [qtype, Question, Answer]\n",
    "------------------------------------------------------------\n",
    "RUN (script):\n",
    "python finetune_qwen25_medqa_cpu.py\n",
    "------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Any, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from packaging import version\n",
    "\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Your paths / defaults\n",
    "# -----------------------------\n",
    "DEFAULT_DATA_PATH = r\"YOUR DIRECTORY to the Dataset\\Medical_QA_Dataset.csv\"\n",
    "DEFAULT_MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "DEFAULT_OUTPUT_DIR = \"./qwen25_1p5b_medqa_lora_cpu\"\n",
    "\n",
    "DEFAULT_MAX_SEQ_LEN = 512\n",
    "DEFAULT_TEST_SIZE = 0.05\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "# LoRA defaults\n",
    "DEFAULT_LORA_R = 8\n",
    "DEFAULT_LORA_ALPHA = 16\n",
    "DEFAULT_LORA_DROPOUT = 0.05\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Safety-oriented system prompt\n",
    "# -----------------------------\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a careful medical information assistant. Provide general educational information, \"\n",
    "    \"not personal medical advice. Encourage consulting qualified clinicians for diagnosis and treatment. \"\n",
    "    \"If symptoms suggest an emergency, advise seeking urgent care. If unsure, say you don't know.\"\n",
    ")\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    IMPORTANT FIX:\n",
    "    - Jupyter/IPykernel passes extra args like: -f <path_to_kernel.json>\n",
    "    - argparse would normally crash.\n",
    "    - We use parse_known_args() and ignore unknown args.\n",
    "    \"\"\"\n",
    "    import argparse\n",
    "\n",
    "    p = argparse.ArgumentParser()\n",
    "\n",
    "    p.add_argument(\"--data_path\", type=str, default=DEFAULT_DATA_PATH)\n",
    "    p.add_argument(\"--model_name\", type=str, default=DEFAULT_MODEL_NAME)\n",
    "    p.add_argument(\"--output_dir\", type=str, default=DEFAULT_OUTPUT_DIR)\n",
    "\n",
    "    p.add_argument(\"--max_seq_len\", type=int, default=DEFAULT_MAX_SEQ_LEN)\n",
    "    p.add_argument(\"--test_size\", type=float, default=DEFAULT_TEST_SIZE)\n",
    "    p.add_argument(\"--seed\", type=int, default=DEFAULT_SEED)\n",
    "\n",
    "    # Training hyperparams (CPU-friendly)\n",
    "    p.add_argument(\"--per_device_train_batch_size\", type=int, default=1)\n",
    "    p.add_argument(\"--per_device_eval_batch_size\", type=int, default=1)\n",
    "    p.add_argument(\"--gradient_accumulation_steps\", type=int, default=16)\n",
    "    p.add_argument(\"--learning_rate\", type=float, default=2e-4)\n",
    "    p.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
    "    p.add_argument(\"--num_train_epochs\", type=float, default=1.0)\n",
    "    p.add_argument(\"--max_steps\", type=int, default=-1)  # set >0 to cap steps for quick test\n",
    "    p.add_argument(\"--warmup_ratio\", type=float, default=0.03)\n",
    "    p.add_argument(\"--lr_scheduler_type\", type=str, default=\"cosine\")\n",
    "\n",
    "    # LoRA hyperparams\n",
    "    p.add_argument(\"--lora_r\", type=int, default=DEFAULT_LORA_R)\n",
    "    p.add_argument(\"--lora_alpha\", type=int, default=DEFAULT_LORA_ALPHA)\n",
    "    p.add_argument(\"--lora_dropout\", type=float, default=DEFAULT_LORA_DROPOUT)\n",
    "\n",
    "    # Data limiting for smoke tests\n",
    "    p.add_argument(\"--max_train_samples\", type=int, default=-1)\n",
    "    p.add_argument(\"--max_eval_samples\", type=int, default=-1)\n",
    "\n",
    "    # Optional: merge adapter into base weights (standalone model)\n",
    "    p.add_argument(\"--merge_model\", action=\"store_true\")\n",
    "\n",
    "    # Optional: quick test generation\n",
    "    p.add_argument(\"--run_test_prompt\", action=\"store_true\")\n",
    "\n",
    "    args, unknown = p.parse_known_args()\n",
    "    if unknown:\n",
    "        print(f\"[INFO] Ignoring unknown argv passed by environment: {unknown}\")\n",
    "    return args\n",
    "\n",
    "\n",
    "def require_min_transformers():\n",
    "    \"\"\"\n",
    "    Qwen2.5 generally needs a reasonably recent Transformers.\n",
    "    If your environment is older, you may see errors like KeyError: 'qwen2'.\n",
    "    \"\"\"\n",
    "    min_ver = version.parse(\"4.37.0\")\n",
    "    cur_ver = version.parse(transformers.__version__)\n",
    "    if cur_ver < min_ver:\n",
    "        raise RuntimeError(\n",
    "            f\"transformers>={min_ver} required for Qwen2.5. You have transformers=={transformers.__version__}. \"\n",
    "            f\"Run: pip install -U transformers\"\n",
    "        )\n",
    "\n",
    "\n",
    "def safe_filter_kwargs_for_callable(callable_obj, kwargs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Keep only kwargs that the callable's signature accepts.\n",
    "    This makes the script robust across Transformers versions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sig = inspect.signature(callable_obj)\n",
    "        accepted = set(sig.parameters.keys())\n",
    "        return {k: v for k, v in kwargs.items() if k in accepted}\n",
    "    except (TypeError, ValueError):\n",
    "        # If signature can't be inspected, return original kwargs\n",
    "        return kwargs\n",
    "\n",
    "\n",
    "def load_and_clean_csv(data_path: str) -> Dataset:\n",
    "    if not os.path.isfile(data_path):\n",
    "        raise FileNotFoundError(f\"CSV not found: {data_path}\")\n",
    "\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    required = [\"qtype\", \"Question\", \"Answer\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in CSV: {missing}. Found columns: {list(df.columns)}\")\n",
    "\n",
    "    df = df[required].copy()\n",
    "    df[\"qtype\"] = df[\"qtype\"].fillna(\"\").astype(str).str.strip()\n",
    "    df[\"Question\"] = df[\"Question\"].fillna(\"\").astype(str).str.strip()\n",
    "    df[\"Answer\"] = df[\"Answer\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "    # Drop empty Q/A\n",
    "    df = df[(df[\"Question\"] != \"\") & (df[\"Answer\"] != \"\")].reset_index(drop=True)\n",
    "    return Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "\n",
    "def build_user_content(qtype: str, question: str) -> str:\n",
    "    if qtype and qtype.lower() not in {\"nan\", \"none\"}:\n",
    "        return f\"Question type: {qtype}\\n\\nQuestion: {question}\"\n",
    "    return question\n",
    "\n",
    "\n",
    "def find_lora_target_modules(model: torch.nn.Module) -> List[str]:\n",
    "    \"\"\"\n",
    "    Auto-detect common projection names for LoRA on Qwen/Llama-like architectures.\n",
    "    \"\"\"\n",
    "    common = {\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"}\n",
    "    found = set()\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            last = name.split(\".\")[-1]\n",
    "            if last in common:\n",
    "                found.add(last)\n",
    "\n",
    "    if not found:\n",
    "        raise RuntimeError(\n",
    "            \"Could not auto-detect LoRA target modules. \"\n",
    "            \"Inspect model.named_modules() and set target_modules manually.\"\n",
    "        )\n",
    "\n",
    "    return sorted(found)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForCausalLMWithLabels:\n",
    "    tokenizer: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self.tokenizer.pad(\n",
    "            {\n",
    "                \"input_ids\": [f[\"input_ids\"] for f in features],\n",
    "                \"attention_mask\": [f[\"attention_mask\"] for f in features],\n",
    "            },\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        max_len = batch[\"input_ids\"].shape[1]\n",
    "        labels = []\n",
    "        for f in features:\n",
    "            lab = f[\"labels\"]\n",
    "            if len(lab) < max_len:\n",
    "                lab = lab + [-100] * (max_len - len(lab))\n",
    "            else:\n",
    "                lab = lab[:max_len]\n",
    "            labels.append(lab)\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
    "        return batch\n",
    "\n",
    "\n",
    "def make_tokenize_fn(tokenizer: Any, max_seq_len: int):\n",
    "    \"\"\"\n",
    "    Creates a function that:\n",
    "    - builds chat-formatted text with tokenizer.apply_chat_template\n",
    "    - tokenizes prompt and full conversation\n",
    "    - masks labels on the prompt so loss is only computed on the assistant answer\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize_example(ex: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        qtype = ex.get(\"qtype\", \"\")\n",
    "        question = ex.get(\"Question\", \"\")\n",
    "        answer = ex.get(\"Answer\", \"\")\n",
    "\n",
    "        user_content = build_user_content(qtype=qtype, question=question)\n",
    "\n",
    "        messages_prompt = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "        ]\n",
    "\n",
    "        prompt_text = tokenizer.apply_chat_template(\n",
    "            messages_prompt,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        messages_full = messages_prompt + [{\"role\": \"assistant\", \"content\": answer}]\n",
    "        full_text = tokenizer.apply_chat_template(\n",
    "            messages_full,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "\n",
    "        prompt_ids = tokenizer(\n",
    "            prompt_text,\n",
    "            add_special_tokens=False,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_len,\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        full = tokenizer(\n",
    "            full_text,\n",
    "            add_special_tokens=False,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_len,\n",
    "        )\n",
    "\n",
    "        input_ids = full[\"input_ids\"]\n",
    "        attention_mask = full[\"attention_mask\"]\n",
    "\n",
    "        labels = input_ids.copy()\n",
    "        prompt_len = min(len(prompt_ids), len(labels))\n",
    "        for i in range(prompt_len):\n",
    "            labels[i] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "    return tokenize_example\n",
    "\n",
    "\n",
    "def maybe_make_output_dir_unique(output_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    If output_dir exists and is non-empty, make a unique directory to avoid collisions,\n",
    "    without deleting anything.\n",
    "    \"\"\"\n",
    "    if os.path.isdir(output_dir) and len(os.listdir(output_dir)) > 0:\n",
    "        stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        new_dir = f\"{output_dir.rstrip('/\\\\\\\\')}_{stamp}\"\n",
    "        print(f\"[INFO] output_dir is not empty. Writing to a new directory:\\n       {new_dir}\")\n",
    "        os.makedirs(new_dir, exist_ok=True)\n",
    "        return new_dir\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "\n",
    "def build_training_arguments(args) -> TrainingArguments:\n",
    "    \"\"\"\n",
    "    Robust TrainingArguments builder across Transformers versions:\n",
    "    - We prepare a superset of args\n",
    "    - Then filter by what's supported in your installed version\n",
    "    \"\"\"\n",
    "\n",
    "    # Some versions may not support overwrite_output_dir.\n",
    "    # To avoid failing or overwriting user data, we just ensure a unique output directory if needed.\n",
    "    args.output_dir = maybe_make_output_dir_unique(args.output_dir)\n",
    "\n",
    "    base_kwargs = {\n",
    "        \"output_dir\": args.output_dir,\n",
    "\n",
    "        # Batch & accumulation\n",
    "        \"per_device_train_batch_size\": args.per_device_train_batch_size,\n",
    "        \"per_device_eval_batch_size\": args.per_device_eval_batch_size,\n",
    "        \"gradient_accumulation_steps\": args.gradient_accumulation_steps,\n",
    "\n",
    "        # Optim schedule\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "        \"num_train_epochs\": args.num_train_epochs,\n",
    "        \"max_steps\": args.max_steps,\n",
    "        \"warmup_ratio\": args.warmup_ratio,\n",
    "        \"lr_scheduler_type\": args.lr_scheduler_type,\n",
    "\n",
    "        # Logging/saving\n",
    "        \"logging_steps\": 10,\n",
    "        \"save_total_limit\": 2,\n",
    "\n",
    "        # Trainer behavior\n",
    "        \"remove_unused_columns\": False,\n",
    "        \"report_to\": \"none\",\n",
    "\n",
    "        # Windows / CPU friendliness\n",
    "        \"dataloader_num_workers\": 0,\n",
    "        \"dataloader_pin_memory\": False,\n",
    "\n",
    "        # Force CPU in most versions\n",
    "        \"no_cuda\": True,     # if supported\n",
    "        \"use_cpu\": True,     # if supported\n",
    "\n",
    "        # Precision flags (should be off on CPU)\n",
    "        \"fp16\": False,\n",
    "        \"bf16\": False,\n",
    "\n",
    "        # Helpful on low-memory (slower but reduces RAM)\n",
    "        \"gradient_checkpointing\": True,\n",
    "\n",
    "        # Optimizer choice (if supported)\n",
    "        \"optim\": \"adamw_torch\",\n",
    "    }\n",
    "\n",
    "    # Add evaluation/save strategy ONLY if supported (names changed in some versions)\n",
    "    ta_sig = inspect.signature(TrainingArguments.__init__)\n",
    "    accepted = set(ta_sig.parameters.keys())\n",
    "\n",
    "    if \"evaluation_strategy\" in accepted:\n",
    "        base_kwargs[\"evaluation_strategy\"] = \"epoch\"\n",
    "    elif \"eval_strategy\" in accepted:\n",
    "        base_kwargs[\"eval_strategy\"] = \"epoch\"\n",
    "\n",
    "    if \"save_strategy\" in accepted:\n",
    "        base_kwargs[\"save_strategy\"] = \"epoch\"\n",
    "\n",
    "    # overwrite_output_dir is optional and apparently not supported in your env.\n",
    "    # Only pass if accepted.\n",
    "    if \"overwrite_output_dir\" in accepted:\n",
    "        base_kwargs[\"overwrite_output_dir\"] = True\n",
    "\n",
    "    filtered = {k: v for k, v in base_kwargs.items() if k in accepted}\n",
    "    return TrainingArguments(**filtered)\n",
    "\n",
    "\n",
    "def build_trainer(model, training_args, train_dataset, eval_dataset, data_collator, tokenizer) -> Trainer:\n",
    "    \"\"\"\n",
    "    Trainer signatures also change across versions (tokenizer arg deprecations).\n",
    "    We pass only what is supported.\n",
    "    \"\"\"\n",
    "    trainer_kwargs = {\n",
    "        \"model\": model,\n",
    "        \"args\": training_args,\n",
    "        \"train_dataset\": train_dataset,\n",
    "        \"eval_dataset\": eval_dataset,\n",
    "        \"data_collator\": data_collator,\n",
    "        # \"tokenizer\": tokenizer,  # only if supported\n",
    "    }\n",
    "\n",
    "    tr_sig = inspect.signature(Trainer.__init__)\n",
    "    accepted = set(tr_sig.parameters.keys())\n",
    "\n",
    "    if \"tokenizer\" in accepted:\n",
    "        trainer_kwargs[\"tokenizer\"] = tokenizer\n",
    "    elif \"processing_class\" in accepted:\n",
    "        # Some newer versions replace \"tokenizer\" with \"processing_class\"\n",
    "        trainer_kwargs[\"processing_class\"] = tokenizer\n",
    "\n",
    "    trainer_kwargs = {k: v for k, v in trainer_kwargs.items() if k in accepted}\n",
    "    return Trainer(**trainer_kwargs)\n",
    "\n",
    "\n",
    "def load_model_cpu(model_name: str):\n",
    "    \"\"\"\n",
    "    Load model on CPU in float32.\n",
    "    Avoids torch_dtype deprecation warnings where possible.\n",
    "    \"\"\"\n",
    "    # Many versions accept torch_dtype; some warn and prefer dtype.\n",
    "    # We'll try dtype first, fall back to torch_dtype.\n",
    "    try:\n",
    "        return AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            dtype=torch.float32,\n",
    "            trust_remote_code=False,\n",
    "        )\n",
    "    except Exception:\n",
    "        return AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float32,\n",
    "            trust_remote_code=False,\n",
    "        )\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    print(f\"[INFO] torch: {torch.__version__}\")\n",
    "    print(f\"[INFO] transformers: {transformers.__version__}\")\n",
    "\n",
    "    require_min_transformers()\n",
    "\n",
    "    set_seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    # Load dataset\n",
    "    raw_ds = load_and_clean_csv(args.data_path)\n",
    "    split = raw_ds.train_test_split(test_size=args.test_size, seed=args.seed)\n",
    "    train_ds = split[\"train\"]\n",
    "    eval_ds = split[\"test\"]\n",
    "\n",
    "    # Optional: limit samples for CPU speed\n",
    "    if args.max_train_samples and args.max_train_samples > 0:\n",
    "        train_ds = train_ds.select(range(min(args.max_train_samples, len(train_ds))))\n",
    "    if args.max_eval_samples and args.max_eval_samples > 0:\n",
    "        eval_ds = eval_ds.select(range(min(args.max_eval_samples, len(eval_ds))))\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name, trust_remote_code=False)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # Model (CPU float32)\n",
    "    model = load_model_cpu(args.model_name)\n",
    "\n",
    "    # Training settings\n",
    "    model.config.use_cache = False\n",
    "    if hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "        model.gradient_checkpointing_enable()\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "\n",
    "    # LoRA\n",
    "    target_modules = find_lora_target_modules(model)\n",
    "    print(f\"[INFO] LoRA target_modules detected: {target_modules}\")\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=args.lora_r,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # Tokenize\n",
    "    tokenize_fn = make_tokenize_fn(tokenizer, args.max_seq_len)\n",
    "    train_tok = train_ds.map(tokenize_fn, remove_columns=train_ds.column_names)\n",
    "    eval_tok = eval_ds.map(tokenize_fn, remove_columns=eval_ds.column_names)\n",
    "\n",
    "    data_collator = DataCollatorForCausalLMWithLabels(tokenizer)\n",
    "\n",
    "    # TrainingArguments (robust)\n",
    "    training_args = build_training_arguments(args)\n",
    "\n",
    "    # Trainer (robust)\n",
    "    trainer = build_trainer(\n",
    "        model=model,\n",
    "        training_args=training_args,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=eval_tok,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    print(\"[INFO] Starting training...\")\n",
    "    train_result = trainer.train()\n",
    "    if hasattr(train_result, \"metrics\"):\n",
    "        trainer.log_metrics(\"train\", train_result.metrics)\n",
    "        trainer.save_metrics(\"train\", train_result.metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "    # Eval\n",
    "    print(\"[INFO] Evaluating...\")\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    if \"eval_loss\" in eval_metrics and eval_metrics[\"eval_loss\"] is not None:\n",
    "        try:\n",
    "            eval_metrics[\"perplexity\"] = math.exp(eval_metrics[\"eval_loss\"])\n",
    "        except OverflowError:\n",
    "            eval_metrics[\"perplexity\"] = float(\"inf\")\n",
    "    trainer.log_metrics(\"eval\", eval_metrics)\n",
    "    trainer.save_metrics(\"eval\", eval_metrics)\n",
    "\n",
    "    # Save LoRA adapter\n",
    "    adapter_dir = os.path.join(training_args.output_dir, \"adapter\")\n",
    "    os.makedirs(adapter_dir, exist_ok=True)\n",
    "    print(f\"[INFO] Saving LoRA adapter to: {adapter_dir}\")\n",
    "    model.save_pretrained(adapter_dir)\n",
    "    tokenizer.save_pretrained(adapter_dir)\n",
    "\n",
    "    # Optional: merge LoRA into base model\n",
    "    if args.merge_model:\n",
    "        print(\"[INFO] Merging LoRA adapter into base model (CPU)...\")\n",
    "        base = load_model_cpu(args.model_name)\n",
    "        merged = PeftModel.from_pretrained(base, adapter_dir)\n",
    "        merged = merged.merge_and_unload()\n",
    "\n",
    "        merged_dir = os.path.join(training_args.output_dir, \"merged_model\")\n",
    "        os.makedirs(merged_dir, exist_ok=True)\n",
    "        print(f\"[INFO] Saving merged model to: {merged_dir}\")\n",
    "        merged.save_pretrained(merged_dir, safe_serialization=True)\n",
    "        tokenizer.save_pretrained(merged_dir)\n",
    "\n",
    "    # Optional: quick generation test\n",
    "    if args.run_test_prompt:\n",
    "        print(\"[INFO] Running a quick generation test...\")\n",
    "        model.eval()\n",
    "\n",
    "        test_question = \"What are common symptoms of influenza (flu), and when should someone seek urgent care?\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": test_question},\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "            )\n",
    "\n",
    "        gen = out[0][inputs[\"input_ids\"].shape[1] :]\n",
    "        print(\"\\n=== MODEL OUTPUT ===\")\n",
    "        print(tokenizer.decode(gen, skip_special_tokens=True))\n",
    "        print(\"====================\\n\")\n",
    "\n",
    "    print(\"[DONE] Training complete.\")\n",
    "    print(f\"[DONE] Adapter saved at: {adapter_dir}\")\n",
    "    if args.merge_model:\n",
    "        print(f\"[DONE] Merged model saved at: {os.path.join(training_args.output_dir, 'merged_model')}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
